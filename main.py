import os
import time
import requests
import urllib.parse
from dotenv import load_dotenv
import html
import json
import email.utils
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

# Telegraph API: https://api.telegra.ph
# access_tokenì€ ìµœì´ˆ 1íšŒ createAccount í›„ ë°œê¸‰/ë³´ê´€
TELEGRAPH_ACCESS_TOKEN = os.getenv("TELEGRAPH_ACCESS_TOKEN")  # â˜… ì¶”ê°€

SEARCH_KEYWORDS_FILE = "search_keywords.txt"
FILTER_KEYWORDS_FILE = "filter_keywords.txt"
LOG_FILE = "sent_log.json"

NEWS_COUNT = 10           # í•œ ë²ˆ ì‹¤í–‰ì— ìµœëŒ€ ì „ì†¡ ìˆ˜
DISPLAY_PER_CALL = 100
MAX_LOOPS = 10
REQUEST_TIMEOUT = 10
PAUSE_BETWEEN_MSGS = 0.5  # í…”ë ˆê·¸ë¨/í…”ë ˆê·¸ë˜í”„ í˜¸ì¶œ ê°„ ì§§ì€ íœ´ì‹

UA = "Mozilla/5.0 (compatible; fcanewsbot/1.0; +https://t.me/)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_keywords(file_path):
    if not os.path.exists(file_path):
        print(f"âš ï¸ í‚¤ì›Œë“œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return []
    with open(file_path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]

def load_sent_log():
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            try:
                return set(json.load(f))
            except Exception:
                return set()
    return set()

def save_sent_log(sent_ids):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(list(sent_ids), f, ensure_ascii=False, indent=2)

def parse_pubdate(pubdate_str):
    try:
        return email.utils.parsedate_to_datetime(pubdate_str)
    except Exception:
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_recent_news(search_keywords, filter_keywords, sent_before):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ API ë°˜ë³µ ìš”ì²­
    - search_keywords : API ê²€ìƒ‰ìš© (AND ì¡°ê±´)
    - filter_keywords : ì œëª© í•„í„°ë§ìš© (OR ì¡°ê±´)
    - ì´ì „ì— ë³´ë‚¸ ê¸°ì‚¬(sent_before)ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
    """
    base_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": CLIENT_ID,
        "X-Naver-Client-Secret": CLIENT_SECRET
    }

    collected = []
    start = 1
    loop_count = 0
    stop_search = False

    while len(collected) < NEWS_COUNT and loop_count < MAX_LOOPS and not stop_search:
        loop_count += 1
        query = " ".join(search_keywords)  # ë„¤ì´ë²„ëŠ” ê³µë°± AND
        url = f"{base_url}?query={urllib.parse.quote(query)}&display={DISPLAY_PER_CALL}&start={start}&sort=date"
        try:
            response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        except Exception as e:
            print("âŒ ìš”ì²­ ì—ëŸ¬:", e)
            break

        if response.status_code != 200:
            print("âŒ ìš”ì²­ ì‹¤íŒ¨:", response.status_code, response.text)
            break

        data = response.json()
        items = data.get("items", [])
        if not items:
            break

        for item in items:
            title_raw = html.unescape(item["title"])
            title_clean = title_raw.replace("<b>", "").replace("</b>", "")
            link = item["link"]

            # ì´ì „ ë¦¬í¬íŠ¸ ê¸°ì‚¬ ë°œê²¬ â†’ ì¤‘ë‹¨
            if link in sent_before:
                stop_search = True
                break

            # ì œëª©ì— í•„í„° í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ì €ì¥
            if any(k.lower() in title_clean.lower() for k in filter_keywords):
                collected.append((title_clean, link))

        start += DISPLAY_PER_CALL
        if start > 1000:
            break

    return collected[:NEWS_COUNT]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_article(url):
    """
    ê°€ëŠ¥í•œ ë²”ìš©ì ìœ¼ë¡œ ì œëª©, ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ.
    - ê°•ê±´ì„±ì„ ìœ„í•´ ì—¬ëŸ¬ í›„ë³´ë¥¼ ì‹œë„
    - ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
    """
    try:
        r = requests.get(url, headers={"User-Agent": UA}, timeout=REQUEST_TIMEOUT)
        if r.status_code != 200 or not r.text:
            return None, None
        soup = BeautifulSoup(r.text, "lxml")

        # ì œëª© í›„ë³´
        title = None
        for sel in ["meta[property='og:title']", "meta[name='title']"]:
            m = soup.select_one(sel)
            if m and m.get("content"):
                title = m["content"].strip()
                break
        if not title and soup.title and soup.title.text:
            title = soup.title.text.strip()

        # ë³¸ë¬¸ í›„ë³´: article, #articleBody, .newsct_article, .article_body ë“± í”í•œ íŒ¨í„´
        candidates = [
            "article",
            "#articleBody",
            ".article_body",
            ".newsct_article",
            ".news_body",
            ".content",
            ".post-content",
            "div[itemprop='articleBody']",
        ]
        body_text = ""
        for css in candidates:
            node = soup.select_one(css)
            if node:
                # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
                for bad in node(["script", "style", "noscript"]):
                    bad.extract()
                text = node.get_text("\n").strip()
                # ë„ˆë¬´ ì§§ìœ¼ë©´ ë¬´ì‹œ
                if len(text) >= 300:
                    body_text = text
                    break

        # ê·¸ë˜ë„ ë¹ˆ ê²½ìš° ê¸°ì‚¬ ì „ì²´ì—ì„œ pë¥¼ ëª¨ì•„ ì‹œë„
        if not body_text:
            ps = soup.find_all("p")
            text = "\n".join(p.get_text(" ").strip() for p in ps if p.get_text().strip())
            if len(text) >= 300:
                body_text = text

        if not title:
            title = "ê¸°ì‚¬ ì œëª©"
        if not body_text:
            return title, None
        return title, body_text
    except Exception as e:
        print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url} / {e}")
        return None, None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Telegraph í˜ì´ì§€ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_telegraph_page(title, body_text, source_url):
    """
    Telegraphì— ê¸°ì‚¬ ì¬ê²Œì‹œ
    - ë³¸ë¬¸ì€ ë‹¨ë½ìœ¼ë¡œ ìª¼ê°œì–´ <p> ë…¸ë“œ ë°°ì—´ë¡œ êµ¬ì„±
    - ë§¨ ì•ì— ì›ë¬¸ ë§í¬ë¥¼ ë„£ì–´ë‘ (ë©”ì‹œì§€ì—ëŠ” ë§í¬ 1ê°œë§Œ ë³´ë‚¼ ê²ƒì´ë¯€ë¡œ, ì›ë¬¸ì€ í˜ì´ì§€ ë‚´ë¶€ ë§í¬ë¡œ ì œê³µ)
    """
    if not TELEGRAPH_ACCESS_TOKEN:
        raise RuntimeError("TELEGRAPH_ACCESS_TOKEN ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ë³¸ë¬¸ ë…¸ë“œ êµ¬ì„±
    nodes = []
    # ì›ë¬¸ ë§í¬ ë‹¨ë½
    nodes.append({
        "tag": "p",
        "children": [
            {"tag": "a", "attrs": {"href": source_url}, "children": ["ì›ë¬¸ ë³´ê¸°"]},
            " Â· ì´ í˜ì´ì§€ëŠ” fcanews ë´‡ì´ ìë™ ë³€í™˜í–ˆìŠµë‹ˆë‹¤."
        ]
    })

    # ë³¸ë¬¸ì„ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ (ì˜ˆ: 6000ì) â€” í…”ë ˆê·¸ë˜í”„ê°€ ë„ˆë¬´ ê¸´ ì½˜í…ì¸ ë¥¼ ì‹«ì–´í•¨
    MAX_CHARS = 6000
    body_trim = body_text[:MAX_CHARS]

    # ë‹¨ë½ ë¶„ë¦¬
    for para in [p.strip() for p in body_trim.split("\n") if p.strip()]:
        nodes.append({"tag": "p", "children": [para]})

    payload = {
        "access_token": TELEGRAPH_ACCESS_TOKEN,
        "title": title[:120],                 # ì œëª© ì œí•œ
        "author_name": "FCAnyang NewsBot",
        "return_content": False,
        "content": json.dumps(nodes, ensure_ascii=False)
    }
    try:
        r = requests.post("https://api.telegra.ph/createPage", data=payload, timeout=REQUEST_TIMEOUT)
        data = r.json()
        if not data.get("ok"):
            raise RuntimeError(f"Telegraph ì˜¤ë¥˜: {data}")
        return data["result"]["url"]
    except Exception as e:
        raise RuntimeError(f"Telegraph ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í…”ë ˆê·¸ë¨ ì „ì†¡ (ë©”ì‹œì§€ë‹¹ ë§í¬ 1ê°œ ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send_to_telegram_single(title, url, is_telegraph=True):
    """
    ë©”ì‹œì§€ í•œ ê±´ ì „ì†¡.
    - ì¦‰ì‹œ ë³´ê¸° ìœ ë„: ë©”ì‹œì§€ì— URL 1ê°œë§Œ í¬í•¨
    - telegraph URLì¸ ê²½ìš° ë¯¸ë¦¬ë³´ê¸° ì¼¬ (disable_web_page_preview=False)
    """
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        print("âš ï¸ TELEGRAM í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False

    send_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    if is_telegraph:
        # ì œëª© + í…”ë ˆê·¸ë˜í”„ ë§í¬ 1ê°œë§Œ
        text = f"ğŸ“° <b>{html.escape(title)}</b>\n{url}"
        disable_preview = False  # ì¦‰ì‹œ ë³´ê¸°/ë¯¸ë¦¬ë³´ê¸° í™œì„±í™”
    else:
        # í´ë°±: ì›ë¬¸ ë§í¬ 1ê°œë§Œ
        text = f"ğŸ“° <b>{html.escape(title)}</b>\n{url}"
        disable_preview = False  # ê·¸ë˜ë„ ì¹´ë“œ ë¯¸ë¦¬ë³´ê¸°ë¼ë„ ë³´ì´ê²Œ

    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": disable_preview
    }
    try:
        r = requests.post(send_url, data=payload, timeout=REQUEST_TIMEOUT)
        ok = r.status_code == 200
        if not ok:
            print("âŒ í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨:", r.text)
        return ok
    except Exception as e:
        print("âŒ í…”ë ˆê·¸ë¨ ì „ì†¡ ì˜ˆì™¸:", e)
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    search_keywords = load_keywords(SEARCH_KEYWORDS_FILE)
    filter_keywords = load_keywords(FILTER_KEYWORDS_FILE)

    if not CLIENT_ID or not CLIENT_SECRET:
        print("âš ï¸ NAVER API í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        print("âš ï¸ TELEGRAM í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    if not TELEGRAPH_ACCESS_TOKEN:
        print("âš ï¸ TELEGRAPH_ACCESS_TOKEN ì´ ì—†ìŠµë‹ˆë‹¤. (ì¦‰ì‹œ ë³´ê¸° ë³€í™˜ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤)")

    if not search_keywords:
        print("âš ï¸ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        exit()
    if not filter_keywords:
        print("âš ï¸ í•„í„°ë§ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    sent_before = load_sent_log()
    found = search_recent_news(search_keywords, filter_keywords, sent_before)

    # ìƒˆ í•­ëª©ë§Œ
    new_items = [(t, l) for (t, l) in found if l not in sent_before]

    if not new_items:
        # ì¢…í•© ë©”ì‹œì§€(ë§í¬ ì—†ìŒ) â€” ì¦‰ì‹œ ë³´ê¸° ê³ ë ¤ ì—†ì´ ì•ˆë‚´ë§Œ
        if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:
            requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage",
                data={
                    "chat_id": TELEGRAM_CHAT_ID,
                    "text": "ğŸ” ìƒˆ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤!",
                    "parse_mode": "HTML",
                    "disable_web_page_preview": True
                },
                timeout=REQUEST_TIMEOUT
            )
        print("ğŸ” ìƒˆ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        exit(0)

    # ê° ê¸°ì‚¬ â†’ Telegraph ì—…ë¡œë“œ â†’ í…”ë ˆê·¸ë¨ ì „ì†¡ (ë©”ì‹œì§€ë‹¹ ë§í¬ 1ê°œ)
    sent_count = 0
    for title, link in new_items:
        telegraph_url = None
        used_telegraph = False

        if TELEGRAPH_ACCESS_TOKEN:
            # 1) ë³¸ë¬¸ ì¶”ì¶œ
            art_title, art_body = extract_article(link)
            if art_title and art_body:
                # 2) Telegraph ìƒì„±
                try:
                    telegraph_url = create_telegraph_page(art_title or title, art_body, link)
                    used_telegraph = True
                except Exception as e:
                    print(f"âš ï¸ Telegraph ì‹¤íŒ¨, ì›ë¬¸ ë§í¬ë¡œ ì „ì†¡í•©ë‹ˆë‹¤: {e}")

        # 3) í…”ë ˆê·¸ë¨ ì „ì†¡ (ë©”ì‹œì§€ì— URL 1ê°œë§Œ)
        if used_telegraph and telegraph_url:
            ok = send_to_telegram_single(title, telegraph_url, is_telegraph=True)
        else:
            ok = send_to_telegram_single(title, link, is_telegraph=False)

        if ok:
            sent_before.add(link)
            sent_count += 1
            time.sleep(PAUSE_BETWEEN_MSGS)  # ì†ë„ ì™„í™”

    # ë¡œê·¸ ì €ì¥
    save_sent_log(sent_before)
    print(f"âœ… ì „ì†¡ ì™„ë£Œ: {sent_count}ê±´")
